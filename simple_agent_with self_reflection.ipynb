{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5661f715",
   "metadata": {},
   "source": [
    "In this example we will enhance an AI Agent with self-reflection capabilities, allowing it to critique its own responses and refine them iteratively. This feature enables the agent to evaluate its own output and improve the response quality before delivering its final answer.\n",
    "\n",
    "Objective\n",
    "\n",
    "The task is to modify the agent so that it can:\n",
    "\n",
    "    -Store conversation history - implement a memory mechanism to track interactions\n",
    "    -Generate an initial response - Process user input and return a response using the language model\n",
    "    -Critique its own response when enabled - if self-reflection is activated, the agent should generate feedback on its own answer\n",
    "    -Refine its response iteratively - Based on the self critique, the agent should adjust its reply, improving clarity, accuracy, and relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c756bf",
   "metadata": {},
   "source": [
    "Steps\n",
    "\n",
    "    1) Imlement a memory layer to retain conversation history\n",
    "    2) Introduce a self-reflection mechanism that allows the agent to analyze its response and refine it\n",
    "    3) Limit the number of self-reflection iterations to prevent excesssive loops (min=1, max=3)\n",
    "    4) Ensure flexibility by allowing users to toggle self-reflection on or off\n",
    "\n",
    "Considerations\n",
    "\n",
    "    -The agent should always generate at least one response before self-reflection.\n",
    "    -If self-reflection is enabled, it should run at least once more to critique and improve its output.\n",
    "    -The number of iterations should be controlled and not exceed three refinements\n",
    "    -Implement logging functionality (verbose mode) to track the refinement process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa538da",
   "metadata": {},
   "source": [
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55cb336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Literal\n",
    "from openai import OpenAI\n",
    "from openai.types.chat.chat_completion_message import ChatCompletionMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e99ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14927f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Answer all questions.\"},\n",
    "                    {\"role\": \"user\", \"content\": \"What have i asked?\"},\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "        )\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4e39f5",
   "metadata": {},
   "source": [
    "Add the memory layer: to add reflection, our agent has to be able to keep track of interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a799237",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = [\n",
    "    {\"role\": \"system\", \"content\": \"Answer all user questions\"},\n",
    "    {\"role\": \"user\", \"content\": \"whats an LLM?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476bf44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=memory,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "memory.append(\n",
    "    {\"role\": \"assistant\", \"content\": new_response.choices[0].message.content}\n",
    ")\n",
    "\n",
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1804687",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.append(\n",
    "    {\"role\": \"user\", \"content\": \"what have i asked?\"}\n",
    ")\n",
    "\n",
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5849fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=memory,\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "memory.append(\n",
    "    {\"role\": \"assistant\", \"content\": new_response.choices[0].message.content}\n",
    ")\n",
    "\n",
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7e8ee",
   "metadata": {},
   "source": [
    "We need to create a proper memory class to deal with more complicated cases\n",
    "\n",
    "We will create the memory class with the following methods:\n",
    "\n",
    "    -add_message\n",
    "    -get_messages\n",
    "    -last_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d4ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self._messages: List[Dict[str, str]] = []\n",
    "\n",
    "    def add_message(self, role: Literal['user', 'system', 'assistant'], content: str):\n",
    "        self._messages.append({\n",
    "            \"role\": role,\n",
    "            \"content\": content\n",
    "        })\n",
    "    \n",
    "    def get_messages(self) -> List[Dict[str, str]]:\n",
    "        return self._messages\n",
    "    \n",
    "    def last_message(self) -> None:\n",
    "        if self._messages:\n",
    "            return self._messages[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbaa617",
   "metadata": {},
   "source": [
    "INVOKE\n",
    "\n",
    "refactor invoke() method. this method should now include:\n",
    "\n",
    "    -self-reflection parameter (default: False)\n",
    "    -max_iter parameter (default: 1)\n",
    "\n",
    "If self_reflection is set to true, it should use a loop to generate an initial response. Then critiquing and refining the response in subsequent iterations up to the number of iterations defined in max_iter.\n",
    "\n",
    "Use the self.memory to store each step\n",
    "\n",
    "Rules for Self-Reflection:\n",
    "\n",
    "    -Dont allow values less than 1\n",
    "    -Dont allow values greater than 3\n",
    "    -Max iter is controlled by self-reflection flag\n",
    "    -If set to true, it needs to call the LLM at least once more for the criticism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9510fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_critique_prompt = \"\"\"\n",
    "Look at your last response and take some time to think about it. Are there any mistakes? Is there a way to improve the response or better ways to clarify\n",
    "the message you are trying to get across?\n",
    "Provide a revised response, if necessary, in a JSON output structure:\n",
    "{\n",
    "    \"original_response\": \"\",\n",
    "    \"revisions_needed\": \"\",\n",
    "    \"updated_response\": \"\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22571444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"A self-reflection AI agent\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name:str = \"Agent\",\n",
    "        role:str = \"Personal Assistant\",\n",
    "        instructions:str = \"Help users with any questions\",\n",
    "        model:str = \"gpt-4o-mini\",\n",
    "        temperature:float = 0.1,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.instructions = instructions\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.client = OpenAI()\n",
    "\n",
    "        self.memory = Memory()\n",
    "        self.memory.add_message(\n",
    "            role=\"system\",\n",
    "            content=f\"Youre an AI agent, your role is {self.role}, \"\n",
    "                f\"and you need to {self.instructions}\",\n",
    "        )\n",
    "\n",
    "        self.critique_prompt = self_critique_prompt\n",
    "\n",
    "    def invoke(self,\n",
    "                user_message: str,\n",
    "                self_reflection: bool = False,\n",
    "                max_iter: int = 1,\n",
    "                verbose: bool = False) -> str:\n",
    "    \n",
    "        self.memory.add_message(\n",
    "            role=\"user\",\n",
    "            content=user_message\n",
    "        )\n",
    "        if verbose:\n",
    "            self._log_last_message()\n",
    "\n",
    "        max_iter = max_iter if max_iter >= 1 else 1\n",
    "        max_iter = max_iter if max_iter <= 3 else 3\n",
    "        max_iter = max_iter if self_reflection else 0.5\n",
    "        loops = 2 * max_iter\n",
    "\n",
    "        for i in range(loops):\n",
    "            ai_message = self._get_completion(\n",
    "                messages = self.memory.get_messages()\n",
    "            )\n",
    "\n",
    "            self.memory.add_message(\n",
    "                role = \"assistant\",\n",
    "                content = ai_message.content,\n",
    "            )\n",
    "            if verbose:\n",
    "                self._log_last_message()\n",
    "\n",
    "            if i < loops - 1:\n",
    "                self.memory.add_message(\n",
    "                    role = \"user\",\n",
    "                    content = self.critique_prompt\n",
    "                )\n",
    "                if verbose:\n",
    "                    self._log_last_message()\n",
    "                \n",
    "                ai_message = self._get_completion(\n",
    "                    messages = self.memory.get_messages()\n",
    "                )\n",
    "\n",
    "    def _get_completion(self, messages:List[Dict])-> ChatCompletionMessage:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            temperature=self.temperature,\n",
    "            messages=messages\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message\n",
    "    \n",
    "    def _log_last_message(self):\n",
    "        print(f\"### {self.memory.last_message()['role']} message ###\\n\".upper())\n",
    "        print(f\"{self.memory.last_message()['content']} \\n\")\n",
    "        print(\"\\n_____________________________________________________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bca44f",
   "metadata": {},
   "source": [
    "Now we can test out some agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064bf46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "agent.invoke(\n",
    "    user_message=\"Who is the greatest basketball player of all time? Lebron James or Michael Jordan?\",\n",
    "    self_reflection=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6dfe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.memory.get_messages()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
